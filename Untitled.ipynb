{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef022af-4060-4830-9577-d87dd45c1c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/raw/file_0.py\n",
      "Saved to data/raw/file_1.py\n",
      "Saved to data/raw/file_2.py\n",
      "Saved to data/raw/file_3.py\n",
      "Saved to data/raw/file_4.py\n",
      "Saved to data/raw/file_5.py\n",
      "Saved to data/raw/file_6.py\n",
      "Saved to data/raw/file_7.py\n",
      "Saved to data/raw/file_8.py\n",
      "Saved to data/raw/file_9.py\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng GitHub API \n",
    "import requests \n",
    "import base64 \n",
    "import os\n",
    "\n",
    "# Thiết lập thông tin API \n",
    "github_token = \"aaaa\" # Tạo token từ GitHub settings headers = { \n",
    "headers = { \n",
    " \"Authorization\": f\"token {github_token}\", \n",
    " \"Accept\": \"application/vnd.github.v3+json\" \n",
    "} \n",
    "\n",
    "# Hàm lấy nội dung file từ repository \n",
    "def get_file_content(owner, repo, path, branch=\"main\"): \n",
    " url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\" \n",
    " response = requests.get(url, headers=headers) \n",
    " if response.status_code == 200: \n",
    "  content = response.json() \n",
    "  \n",
    "  # Kiểm tra nếu là file \n",
    "  if \"type\" in content and content[\"type\"] == \"file\":  \n",
    "    return base64.b64decode(content[\"content\"]).decode(\"utf-8\")\n",
    "  \n",
    "  return None \n",
    "\n",
    "# Lấy danh sách các file Python trong một repository \n",
    "def get_python_files(owner, repo, path=\"\", branch=\"main\"):  \n",
    " url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\" \n",
    " response = requests.get(url, headers=headers) \n",
    " files = [] \n",
    "  \n",
    " if response.status_code == 200: \n",
    "  contents = response.json()\n",
    "  for item in contents: \n",
    "    if item[\"type\"] == \"file\" and item[\"name\"].endswith(\".py\"):\n",
    "      files.append(item[\"path\"]) \n",
    "    elif item[\"type\"] == \"dir\": \n",
    "    # Đệ quy cho thư mục con \n",
    "      files.extend(get_python_files(owner, repo, item[\"path\"], branch)) \n",
    "  \n",
    "  return files \n",
    "\n",
    "owner = \"tensorflow\" \n",
    "repo = \"models\" \n",
    "python_files = get_python_files(owner, repo, path=\"official/legacy/bert\",  branch=\"master\")\n",
    "\n",
    "# Lưu các file vào thư mục local \n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "for i, file_path in enumerate(python_files[:10]): # Lấy 10 file đầu  tiên \n",
    " content = get_file_content(owner, repo, file_path, branch=\"master\")\n",
    " if content: \n",
    "  local_path = f\"data/raw/file_{i}.py\" \n",
    "  with open(local_path, \"w\", encoding=\"utf-8\") as f:  f.write(content) \n",
    "  print(f\"Saved to {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43661fc1-adb7-4188-a002-be856f352057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 53 functions\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "import re \n",
    "def preprocess_python_code(code): \n",
    " # Loại bỏ comments \n",
    " code = re.sub(r'#.*', '', code) \n",
    " code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"', '', code) \n",
    " code = re.sub(r\"'''[\\s\\S]*?'''\", '', code) \n",
    "  \n",
    " # Chuẩn hóa khoảng trắng \n",
    " code = re.sub(r'\\s+', ' ', code) \n",
    "  \n",
    " return code.strip() \n",
    "def extract_functions(code): \n",
    " try: \n",
    "  tree = ast.parse(code) \n",
    "  functions = [] \n",
    "  \n",
    "  for node in ast.walk(tree): \n",
    "    if isinstance(node, ast.FunctionDef): \n",
    "      func_code = ast.get_source_segment(code, node)\n",
    "      functions.append({ \n",
    "        'name': node.name, \n",
    "        'code': func_code, \n",
    "        'processed_code': preprocess_python_code(func_code) \n",
    "      }) \n",
    "  \n",
    "  return functions \n",
    " except SyntaxError: \n",
    "  return [] \n",
    " \n",
    "# Ví dụ sử dụng \n",
    "import os \n",
    "processed_data = [] \n",
    "for i in range(10): # Cho 10 file đã tải \n",
    " file_path = f\"data/raw/file_{i}.py\"\n",
    " if os.path.exists(file_path): \n",
    "  with open(file_path, \"r\", encoding=\"utf-8\") as f:  code = f.read() \n",
    "\n",
    "  # Tiền xử lý và trích xuất hàm \n",
    "  functions = extract_functions(code) \n",
    "  processed_data.extend(functions) \n",
    "\n",
    "print(f\"Extracted {len(processed_data)} functions\") \n",
    "# Lưu dữ liệu đã xử lý \n",
    "\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(processed_data) \n",
    "df.to_csv(\"data/processed/functions.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a643a60c-9922-4e6d-9b12-16f960e0d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (53, 5000)\n",
      "    02  02 embedding_size  02 embedding_size none  02 intermediate_size  \\\n",
      "0  0.0                0.0                     0.0                   0.0   \n",
      "1  0.0                0.0                     0.0                   0.0   \n",
      "2  0.0                0.0                     0.0                   0.0   \n",
      "3  0.0                0.0                     0.0                   0.0   \n",
      "4  0.0                0.0                     0.0                   0.0   \n",
      "\n",
      "   02 intermediate_size 32   10  10 dummy_ids  10 dummy_ids np  100  \\\n",
      "0                      0.0  0.0           0.0              0.0  0.0   \n",
      "1                      0.0  0.0           0.0              0.0  0.0   \n",
      "2                      0.0  0.0           0.0              0.0  0.0   \n",
      "3                      0.0  0.0           0.0              0.0  0.0   \n",
      "4                      0.0  0.0           0.0              0.0  0.0   \n",
      "\n",
      "   100 dataset  ...  zip bert_model  zip bert_model trainable_weights  \\\n",
      "0          0.0  ...             0.0                               0.0   \n",
      "1          0.0  ...             0.0                               0.0   \n",
      "2          0.0  ...             0.0                               0.0   \n",
      "3          0.0  ...             0.0                               0.0   \n",
      "4          0.0  ...             0.0                               0.0   \n",
      "\n",
      "   zip clipped_grads  zip clipped_grads variables  zip grads  \\\n",
      "0                0.0                          0.0        0.0   \n",
      "1                0.0                          0.0        0.0   \n",
      "2                0.0                          0.0        0.0   \n",
      "3                0.0                          0.0        0.0   \n",
      "4                0.0                          0.0        0.0   \n",
      "\n",
      "   zip grads training_vars  zip grads_and_vars  \\\n",
      "0                      0.0                 0.0   \n",
      "1                      0.0                 0.0   \n",
      "2                      0.0                 0.0   \n",
      "3                      0.0                 0.0   \n",
      "4                      0.0                 0.0   \n",
      "\n",
      "   zip grads_and_vars clipped_grads  zip outputs  zip outputs labels  \n",
      "0                               0.0          0.0                 0.0  \n",
      "1                               0.0          0.0                 0.0  \n",
      "2                               0.0          0.0                 0.0  \n",
      "3                               0.0          0.0                 0.0  \n",
      "4                               0.0          0.0                 0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n",
      "Successfully saved vectorizer to models/tfidf_vectorizer.pkl\n",
      "Successfully saved TF-IDF matrix to data/processed/tfidf_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Đọc dữ liệu đã xử lý \n",
    "df = pd.read_csv(\"data/processed/functions.csv\")\n",
    "\n",
    "# Khởi tạo TF-IDF vectorizer \n",
    "tfidf = TfidfVectorizer( \n",
    " max_features=5000, # Giới hạn số lượng từ \n",
    " ngram_range=(1, 3), # Sử dụng unigram, bigram và trigram  stop_words='english' # Loại bỏ stopwords \n",
    ") \n",
    "# Tạo ma trận TF-IDF \n",
    "tfidf_matrix = tfidf.fit_transform(df['processed_code']) \n",
    "\n",
    "# Chuyển ma trận thành DataFrame để dễ xem \n",
    "tfidf_df = pd.DataFrame( \n",
    " tfidf_matrix.toarray(), \n",
    " columns=tfidf.get_feature_names_out() \n",
    ") \n",
    "\n",
    "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "print(tfidf_df.head()) \n",
    "\n",
    "# Tạo tất cả các thư mục cần thiết\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Lưu vectorizer để tái sử dụng \n",
    "try:\n",
    "    with open(\"models/tfidf_vectorizer.pkl\", \"wb\") as f: \n",
    "        pickle.dump(tfidf, f)\n",
    "    print(\"Successfully saved vectorizer to models/tfidf_vectorizer.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving vectorizer: {e}\")\n",
    "\n",
    "# Lưu ma trận TF-IDF \n",
    "try:\n",
    "    np.save(\"data/processed/tfidf_matrix.npy\", tfidf_matrix.toarray())\n",
    "    print(\"Successfully saved TF-IDF matrix to data/processed/tfidf_matrix.npy\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving TF-IDF matrix: {e}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2415688-60b7-4cbe-8bb7-c9e3dc744468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded NLTK punkt tokenizer\n",
      "Loaded 53 code samples\n",
      "Shape of Word2Vec embeddings: (53, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/toan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Try to download NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    print(\"Downloaded NLTK punkt tokenizer\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "    print(\"If you encounter issues with NLTK, try running these commands manually:\")\n",
    "    print(\"import nltk\")\n",
    "    print(\"nltk.download('punkt')\")\n",
    "\n",
    "# Load processed data\n",
    "df = pd.read_csv(\"data/processed/functions.csv\")\n",
    "print(f\"Loaded {len(df)} code samples\")\n",
    "\n",
    "# Chuẩn bị dữ liệu cho Word2Vec \n",
    "tokenized_code = [] \n",
    "for code in df['processed_code']: \n",
    "    tokens = word_tokenize(str(code)) \n",
    "    tokenized_code.append(tokens) \n",
    "\n",
    "# Huấn luyện mô hình Word2Vec \n",
    "w2v_model = Word2Vec( \n",
    "    sentences=tokenized_code, \n",
    "    vector_size=100, # Kích thước vector \n",
    "    window=5, # Kích thước cửa sổ ngữ cảnh \n",
    "    min_count=2, # Tối thiểu số lần xuất hiện của từ\n",
    "    workers=4 # Số luồng \n",
    ") \n",
    "\n",
    "# Lưu mô hình \n",
    "w2v_model.save(\"models/w2v_code.model\") \n",
    "\n",
    "# Tạo embedding cho mỗi hàm bằng cách lấy trung bình các vector từ\n",
    "def create_document_vector(doc_tokens, model): \n",
    "    doc_vector = [] \n",
    "    for token in doc_tokens: \n",
    "        if token in model.wv: \n",
    "            doc_vector.append(model.wv[token]) \n",
    "  \n",
    "    if not doc_vector: \n",
    "        return np.zeros(model.vector_size) \n",
    "    return np.mean(doc_vector, axis=0) \n",
    "\n",
    "# Tạo embedding cho mỗi hàm \n",
    "doc_vectors = [] \n",
    "for tokens in tokenized_code: \n",
    "    doc_vectors.append(create_document_vector(tokens, w2v_model))\n",
    "\n",
    "# Lưu các vector \n",
    "doc_vectors_array = np.array(doc_vectors) \n",
    "np.save(\"data/processed/w2v_vectors.npy\", doc_vectors_array) \n",
    "print(f\"Shape of Word2Vec embeddings: {doc_vectors_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed81814-be30-459b-ae7b-c8ca1d31eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "# Đọc dữ liệu vector \n",
    "tfidf_vectors = np.load(\"data/processed/tfidf_matrix.npy\")\n",
    "w2v_vectors = np.load(\"data/processed/w2v_vectors.npy\") \n",
    "# Giả định: Gán nhãn cho mỗi hàm (ví dụ: phân loại theo chức năng) \n",
    "# Trong thực tế, bạn cần có dữ liệu đã được gán nhãn \n",
    "# Ở đây, chúng ta tạo nhãn giả cho mục đích demo \n",
    "df = pd.read_csv(\"data/processed/functions.csv\") \n",
    "# Ví dụ: Phân loại hàm theo tên \n",
    "# 0: hàm bắt đầu bằng \"get_\" hoặc \"fetch_\" \n",
    "# 1: hàm bắt đầu bằng \"create_\" hoặc \"build_\" \n",
    "# 2: các hàm còn lại \n",
    "def assign_label(func_name): \n",
    " if func_name.startswith(('get_', 'fetch_')): \n",
    "  return 0 \n",
    " elif func_name.startswith(('create_', 'build_')): \n",
    "  return 1 \n",
    " else: \n",
    "  return 2 \n",
    "df['label'] = df['name'].apply(assign_label) \n",
    "# Chia dữ liệu \n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(  tfidf_vectors, df['label'], test_size=0.3, random_state=42) \n",
    "X_train_w2v, X_test_w2v, _, _ = train_test_split(w2v_vectors, df['label'], test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a3ea31-8b9b-44bb-bca9-ad07f7442dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM với TF-IDF:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train) \n",
    "# Dự đoán \n",
    "y_pred = svm_model.predict(X_test_tfidf) \n",
    "# Đánh giá \n",
    "print(\"SVM với TF-IDF:\") \n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0)) \n",
    "# Lưu mô hình \n",
    "import pickle \n",
    "with open(\"models/svm_tfidf.pkl\", \"wb\") as f: \n",
    "  pickle.dump(svm_model, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf6e3f7-aee2-4f3b-bda1-d53bebe075d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest với Word2Vec:\n",
      "Accuracy: 0.8125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.81      0.90        16\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.50      0.41      0.45        16\n",
      "weighted avg       1.00      0.81      0.90        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rf_model = RandomForestClassifier( \n",
    " n_estimators=100, \n",
    " max_depth=10, \n",
    " random_state=42 \n",
    ") \n",
    "rf_model.fit(X_train_w2v, y_train) \n",
    "# Dự đoán \n",
    "y_pred = rf_model.predict(X_test_w2v) \n",
    "# Đánh giá \n",
    "print(\"\\nRandom Forest với Word2Vec:\") \n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0)) \n",
    "# Lưu mô hình \n",
    "with open(\"models/rf_w2v.pkl\", \"wb\") as f: \n",
    " pickle.dump(rf_model, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7f4c7c-ae3b-4068-b64e-3b91abf85399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest với TF-IDF:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n",
      "\n",
      "So sánh biểu diễn vector:\n",
      "SVM + TF-IDF: 1.0000\n",
      "RF + Word2Vec: 0.8125\n",
      "RF + TF-IDF: 1.0000\n"
     ]
    }
   ],
   "source": [
    "rf_tfidf = RandomForestClassifier( \n",
    " n_estimators=100,\n",
    " max_depth=10, \n",
    " random_state=42 \n",
    ") \n",
    "rf_tfidf.fit(X_train_tfidf, y_train) \n",
    "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf) \n",
    "# Đánh giá \n",
    "print(\"\\nRandom Forest với TF-IDF:\") \n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(classification_report(y_test, y_pred_tfidf, zero_division=0)) \n",
    "# Kết luận \n",
    "print(\"\\nSo sánh biểu diễn vector:\") \n",
    "print(f\"SVM + TF-IDF: {accuracy_score(y_test, svm_model.predict(X_test_tfidf)):.4f}\") \n",
    "print(f\"RF + Word2Vec: {accuracy_score(y_test,  rf_model.predict(X_test_w2v)):.4f}\") \n",
    "print(f\"RF + TF-IDF: {accuracy_score(y_test, y_pred_tfidf):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc9f92-ef86-48d0-bf61-fc330cf82bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
